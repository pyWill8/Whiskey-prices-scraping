{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Page 33 of 33\n",
    "# or\n",
    "# 500 per page\n",
    "# so thinking do an if statement on the last value of that string, if its an int use it as the number of pages, if not just take that page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bottles(bottles):\n",
    "\n",
    "    bottle_ids = bottles.find_all(\"h6\")                                      \n",
    "    bottle_names = bottles.find_all(\"h4\")[3:]\n",
    "    all_bottles_prices = bottles.find_all(class_= [\"sold\", \"bndone\"])\n",
    "\n",
    "    put_together = list(zip(bottle_ids, bottle_names, all_bottles_prices)) # putting all the info into a list\n",
    "\n",
    "    df = pd.DataFrame(put_together, columns=[\"Lot number\", \"Name\", \"Price\"])\n",
    "    for i in df.columns:\n",
    "        df[i] = df[i].apply(lambda x: BeautifulSoup(str(x), \"html.parser\").get_text())\n",
    "    df = df.loc[~df[\"Price\"].str.contains(\"This lot is no longer available\")]  # Getting just sold bottles\n",
    "\n",
    "    return df\n",
    "\n",
    "#bottles = soup\n",
    "#df = find_bottles(bottles)\n",
    "#df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auctions/14-the-inaugural-auction/\n"
     ]
    }
   ],
   "source": [
    "link2 = \"https://www.scotchwhiskyauctions.com/auctions/\" \n",
    "webpage_response2 = requests.get(link2)\n",
    "\n",
    "webpage2 = webpage_response2.content\n",
    "soup2 = BeautifulSoup(webpage2, \"lxml\")\n",
    "\n",
    "auction_links = [i[\"href\"] for i in soup2.find_all(\"a\", class_=\"auction\", href=True)] # Getting the auction links\n",
    "auction_links.reverse()\n",
    "print(auction_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Auction number: 1\n",
      "Scraped: 1 pages\n",
      "-----\n",
      "Auction number: 2\n",
      "Scraped: 2 pages\n",
      "-----\n",
      "Auction number: 3\n",
      "Scraped: 3 pages\n",
      "-----\n",
      "Auction number: 4\n",
      "Scraped: 4 pages\n",
      "-----\n",
      "Auction number: 5\n",
      "Scraped: 5 pages\n",
      "-----\n",
      "Auction number: 6\n",
      "Scraped: 6 pages\n",
      "-----\n",
      "Auction number: 7\n",
      "Scraped: 7 pages\n",
      "-----\n",
      "Auction number: 8\n",
      "Scraped: 8 pages\n",
      "-----\n",
      "Auction number: 9\n",
      "Scraped: 9 pages\n",
      "-----\n",
      "Auction number: 10\n",
      "Scraped: 10 pages\n",
      "-----\n",
      "Auction number: 11\n",
      "Scraped: 11 pages\n",
      "-----\n",
      "Auction number: 12\n",
      "Scraped: 12 pages\n",
      "-----\n",
      "Auction number: 13\n",
      "Scraped: 13 pages\n",
      "-----\n",
      "Auction number: 14\n",
      "Scraped: 14 pages\n",
      "-----\n",
      "Auction number: 15\n",
      "Scraped: 15 pages\n",
      "Scraped: 16 pages\n",
      "-----\n",
      "Auction number: 16\n",
      "Scraped: 17 pages\n",
      "-----\n",
      "Auction number: 17\n",
      "Scraped: 18 pages\n",
      "-----\n",
      "Auction number: 18\n",
      "Scraped: 19 pages\n",
      "-----\n",
      "Auction number: 19\n",
      "Scraped: 20 pages\n",
      "-----\n",
      "Auction number: 20\n",
      "Scraped: 21 pages\n",
      "-----\n",
      "Auction number: 21\n",
      "Scraped: 22 pages\n",
      "-----\n",
      "Auction number: 22\n",
      "Scraped: 23 pages\n",
      "Scraped: 24 pages\n",
      "-----\n",
      "Auction number: 23\n",
      "Scraped: 25 pages\n",
      "Scraped: 26 pages\n",
      "-----\n",
      "Auction number: 24\n",
      "Scraped: 27 pages\n",
      "Scraped: 28 pages\n",
      "Scraped: 29 pages\n",
      "Scraped: 30 pages\n",
      "-----\n",
      "Auction number: 25\n"
     ]
    }
   ],
   "source": [
    "#link = \"https://www.scotchwhiskyauctions.com/auctions/181-the-137th-auction/?page=1\"\n",
    "#webpage_response =requests.get(link)\n",
    "#\n",
    "#webpage = webpage_response.content\n",
    "#soup = BeautifulSoup(webpage, \"lxml\")\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Lot number\", \"Name\", \"Price\"])\n",
    "count = 0\n",
    "auction_count = 0\n",
    "if __name__ == \"__main__\":\n",
    "    # Iterating through all the auctions\n",
    "    for auction in auction_links:\n",
    "        print(\"-----\") # Make the terminal a bit easier to read\n",
    "        auction_count += 1\n",
    "        print(f\"Auction number: {auction_count}\")\n",
    "        \n",
    "        webpage_response = requests.get(\"https://www.scotchwhiskyauctions.com/\" + auction + \"?q=macallan&search=c\") #   Getting a auction page\n",
    "\n",
    "        webpage = webpage_response.content\n",
    "        soup = BeautifulSoup(webpage, \"lxml\")\n",
    "\n",
    "        # Getting the number of pages for each auction\n",
    "        number_pages_tag = soup.find_all(\"option\")\n",
    "        total_auction_pages_list = [i.text for i in number_pages_tag][-3].split() # Getting the pages string, and putting in a list to get the last value\n",
    "        \n",
    "        number_of_pages = total_auction_pages_list[-1] # Getting the pages last value (being the total number of pages on that auction, or \"page\" if there was only one page)\n",
    "        \n",
    "        \n",
    "        try: \n",
    "            for page in range(1, int(number_of_pages)): # Need the number of pages for each auction\n",
    "                link = \"https://www.scotchwhiskyauctions.com/\"\n",
    "                link = link + auction + \"?q=macallan&search=c\" + \"&page=\" + str(page)\n",
    "                webpage_response = requests.get(link)\n",
    "\n",
    "\n",
    "                webpage = webpage_response.content\n",
    "                soup = BeautifulSoup(webpage, \"lxml\")\n",
    "\n",
    "                current_page = find_bottles(soup)\n",
    "                df = pd.concat([df, current_page], ignore_index=True)\n",
    "                #print(current_page.values)\n",
    "\n",
    "                time.sleep(1) \n",
    "                count += 1\n",
    "                print(f\"Scraped: {count} pages\")\n",
    "        except:\n",
    "            link = \"https://www.scotchwhiskyauctions.com/\"\n",
    "\n",
    "            # In this one we dont have the page number\n",
    "            link = link + auction + \"?q=macallan&search=c\"    \n",
    "            webpage_response = requests.get(link)\n",
    "            webpage = webpage_response.content\n",
    "            soup = BeautifulSoup(webpage, \"lxml\")\n",
    "\n",
    "            current_page = find_bottles(soup)\n",
    "            df = pd.concat([df, current_page], ignore_index=True)\n",
    "            #print(current_page.values)\n",
    "\n",
    "            time.sleep(1) \n",
    "            count += 1\n",
    "            print(f\"Scraped: {count} pages\")\n",
    "            \n",
    "\n",
    "\n",
    "df[\"Price\"] = df[\"Price\"].str.replace(\",\", \"\") # Removing the commas in the price column\n",
    "df[\"Price\"] = df[\"Price\"].str.extract(\"Sold for Â£(\\d+)\") #Capturing just the prices. \n",
    "    \n",
    "df[\"Price\"] = df[\"Price\"].astype(int) # Putting the prices as an int\n",
    "#df.drop(columns=[\"Unnamed: 0\"], inplace=True) # Dropping the other index as a column\n",
    "\n",
    "#df.to_csv(\"Macallan 75th.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get all the links so we can iterate through\n",
    "# We then need iterate through each of the links and each of the pages using an imbedded for loop\n",
    "# We also need to figure out a way to update the page part of the for loop so that we automatically change the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e840f835684bb817df5906e8ed510f836174b639f32d5b8a3998d784828aaa2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
